# üîç Diagn√≥stico Completo - Dashboard Facto$

**Fecha:** 8 de febrero de 2025  
**Versi√≥n analizada:** app.py (~2436 l√≠neas), db.py, prep.py

---

## üìã Resumen Ejecutivo

Se identificaron **errores fundamentales** en el manejo de claves √∫nicas, la carga de CSV y la consistencia entre m√≥dulos. Las correcciones aplicadas abordan las causas ra√≠z de ca√≠das, duplicados y fallos con archivos bancarios.

---

## üö® Errores Cr√≠ticos Encontrados y Corregidos

### 1. **unique_key inconsistente (CR√çTICO - CAUSA DE DUPLICADOS Y REINGESTA)**

**Problema:** `app.py` usaba `hash()` de Python para generar `unique_key`:
```python
return f"h:{hash((fstr, mc_val, d))}"  # ‚ùå hash() NO es determin√≠stico entre sesiones
```
- En Python 3.3+, `hash()` incluye sal para seguridad ‚Üí **var√≠a entre reinicios**
- `db.py` usa `hashlib.sha256` ‚Üí formato `k:xxx` (determin√≠stico)
- **Consecuencia:** Las tombstones (filas borradas) usaban claves `k:xxx`, pero el CSV generaba `h:xxx` ‚Üí las filas eliminadas volv√≠an a aparecer al subir de nuevo el CSV.

**Correcci√≥n:** Uso exclusivo del algoritmo de `db._compute_unique_key_row` v√≠a `compute_unique_keys_for_df()` antes del filtro de tombstones y upsert.

---

### 2. **Filtro de Tombstones con claves incorrectas**

**Problema:** Se filtraban tombstones usando `unique_key` generado por `hash()` en lugar del can√≥nico de la BD.

**Correcci√≥n:** Se llama a `compute_unique_keys_for_df()` antes del filtro de tombstones para alinear claves con la BD.

---

### 3. **Carga CSV fr√°gil (encoding y formato)**

**Problemas:**
- Sin detecci√≥n de encoding ‚Üí archivos Latin-1/CP1252 de bancos chilenos fallaban
- Columnas fijas (`fecha`, `detalle`, `monto`) ‚Üí no reconoc√≠a `glosa`, `descripcion`, `cargo`, `importe`
- Formato de fecha r√≠gido ‚Üí `DD/MM/YYYY` mal interpretado
- Sin `on_bad_lines="skip"` ‚Üí una l√≠nea corrupta tiraba todo el proceso

**Correcciones:**
- Detecci√≥n de encoding con `chardet`
- Aliases de columnas: `glosa`‚Üí`detalle`, `cargo`‚Üí`monto`, `fraccion_mia`‚Üí`fraccion_mia_sugerida`, etc.
- `pd.to_datetime(..., dayfirst=True)` para fechas chilenas
- Fallback con `on_bad_lines="skip"` en lectura CSV

---

### 4. **Incompatibilidad con prep.py**

**Problema:** `prep.py` genera `fraccion_mia`, `monto_mio`, `id` (sha1); la app esperaba `fraccion_mia_sugerida`, `monto_mio_estimado`.

**Correcci√≥n:** Mapeo de columnas en `load_df` y en `upsert_transactions` para aceptar ambos formatos.

---

## ‚ö†Ô∏è Problemas de Rendimiento (identificados, no corregidos por complejidad)

| √Årea | Situaci√≥n | Recomendaci√≥n |
|------|-----------|---------------|
| **load_all** | Carga toda la tabla sin l√≠mite | Paginaci√≥n o filtro por rango de fechas por defecto |
| **upsert_transactions** | Inserci√≥n fila por fila en Postgres | Batch insert con `executemany` o COPY |
| **build_suggestions_df** | Uso de `iterrows()` | Vectorizaci√≥n con merge/groupby |
| **Dashboard** | Varios gr√°ficos Altair por carga | Lazy rendering o tabs para gr√°ficos |
| **st.cache_data(load_df)** | Cache por archivo puede ser inefectivo | TTL corto o invalidaci√≥n expl√≠cita tras subida |

---

## üîß Recomendaciones de Arquitectura

### Corto plazo (ya implementadas)
- [x] unique_key can√≥nico en todo el flujo
- [x] Tombstones con claves correctas
- [x] Encoding y columnas flexibles en CSV
- [x] Compatibilidad con prep.py

### Medio plazo
1. **Migrar carga directa:** Permitir subir CSV crudo de banco y procesar con l√≥gica de `prep.py` integrada en la app.
2. **√çndices DB:** Revisar que `idx_movimientos_fecha`, `idx_movimientos_categoria` est√©n en uso.
3. **L√≠mite de carga:** Mostrar primeras N filas en tablas y gr√°ficos; opci√≥n "cargar m√°s".
4. **Validaci√≥n pre-upload:** Preview del CSV antes de insertar (columnas detectadas, muestra, advertencias).

### Largo plazo
1. **API REST:** Separar backend (FastAPI/Flask) de Streamlit para mejor escalabilidad.
2. **Jobs as√≠ncronos:** Subir CSV ‚Üí procesamiento en background ‚Üí notificaci√≥n al terminar.
3. **Tests:** Suite de tests para `load_df`, `upsert_transactions`, `compute_unique_keys_for_df`.

---

## üìÅ Archivos Modificados

| Archivo | Cambios |
|---------|---------|
| `app.py` | `compute_unique_keys_for_df` antes de tombstone; encoding y aliases en `load_df`; fechas flexibles; import `io` |
| `db.py` | Nueva funci√≥n `compute_unique_keys_for_df`; alias `fraccion_mia`/`monto_mio` en upsert |

---

## ‚úÖ Checklist Post-Deploy

- [ ] Subir CSV de banco (Latin-1, punto y coma) y confirmar carga correcta
- [ ] Borrar una transacci√≥n y volver a subir el mismo CSV ‚Üí no debe reaparecer
- [ ] Usar CSV generado por `prep.py` ‚Üí debe procesarse sin errores
- [ ] Revisar que no haya duplicados por `unique_key` en la BD

---

*Diagn√≥stico realizado tras revisi√≥n completa del c√≥digo. Las correcciones priorizan estabilidad y compatibilidad sobre optimizaciones de rendimiento.*
